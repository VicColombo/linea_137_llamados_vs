{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos a usar\n",
    "\n",
    "- V2 --> dataset limpio CAPAZ DESPUÉS\n",
    "- V5 --> eliminación de las variables cn poca información\n",
    "\n",
    "\n",
    "##### Pipeline\n",
    "\n",
    "1. cargar dataset\n",
    "2. reemplazar NS/NC en convive por NA\n",
    "3. hacer modificaciones para svm. Considerar: Apply scikit's OneHotEncoder with the handle_unknown parameter set to \"ignore\"\n",
    "4. separar NA de convive, serán el dataset \"no visto\"\n",
    "5. separar features de target\n",
    "6. separar train-test NO DEBERÍA HACER LAS MODIFICACIONES EN TRAIN Y TEST POR SEPARADO?\n",
    "7. armar modelo\n",
    "8. evaluar\n",
    "\n",
    "\n",
    "##### Modificaiones para SVM\n",
    "\n",
    "1. pasar datetime a timestamp y escalar --> OK\n",
    "2. escalar la variable edad --> OK\n",
    "3. pasar edad a categórica (en una columna distinta) --> OK\n",
    "4. pasar todas las categóricas a dummy:\n",
    "   1. edad, momento del día y estación del año con un encoder ordinal\n",
    "   2. escalar las features encodeadas con ordinal\n",
    "   3. variables de violencia SI/NO para 1 y 0\n",
    "   4. el resto con one hot encoder\n",
    "5. borrar las categóricas para hacer más pequeño el dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. cargar datasets\n",
    "\n",
    "#llamados_v2= pd.read_excel(os.path.join(dataset_dir, 'xlsx/llamados_v2.xlsx'), parse_dates=['llamado_fecha_hora'])\n",
    "#llamados_v4= pd.read_excel(os.path.join(dataset_dir, 'xlsx/llamados_v4.xlsx'), parse_dates=['llamado_fecha_hora'])\n",
    "llamados_v5= pd.read_excel(os.path.join(dataset_dir, 'xlsx/llamados_v5.xlsx'), parse_dates=['llamado_fecha_hora'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todos_datasets = [llamados_v2, llamados_v4, llamados_v5]\n",
    "#datasets_45 = [llamados_v4, llamados_v5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 reemplazar todos los NS/NC en convive por na\n",
    "\n",
    "llamados_v5.loc[:, 'victima_convive_agresor'] = llamados_v5['victima_convive_agresor'].replace({'NS/NC': pd.NA})\n",
    "\n",
    "#for dataset in todos_datasets:\n",
    "\n",
    "    # Replace the specified values with NaN\n",
    "#    dataset.loc[:, 'victima_convive_agresor'] = dataset['victima_convive_agresor'].replace({'NS/NC': pd.NA})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Modificaciones para SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1        0.000060\n",
       "2        0.000121\n",
       "3        0.000181\n",
       "4        0.000181\n",
       "           ...   \n",
       "19138    0.999758\n",
       "19139    0.999940\n",
       "19140    0.999819\n",
       "19141    0.999879\n",
       "19142    0.999879\n",
       "Name: timestamp_encoded_scaled, Length: 19143, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. timestamp-fecha_hora encodeado y escalado\n",
    "scaler = MinMaxScaler()\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "\n",
    "llamados_v5['timestamp_encoded'] = encoder.fit_transform(llamados_v5[['llamado_fecha_hora']])\n",
    "llamados_v5['timestamp_encoded_scaled'] = scaler.fit_transform(llamados_v5[['timestamp_encoded']])\n",
    "    \n",
    "llamados_v5.timestamp_encoded_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Edad. A edad le hago dos cosas para dos pruebas distintas: \n",
    "# cosa 1. escalo los casos completos.\n",
    "# cosa 2: la paso a categórica, ahí los faltantes quedan codificados como una categoría más, y ahí luego la encodeo como \n",
    "# variable categórica con un ordina encoder para preservar el orden. \n",
    "\n",
    "\n",
    "\n",
    "# Cosa 1: edad escalada para pruebas con casos completos de edad\n",
    "# printear una edad faltante a ver qué hace con esos faltantes\n",
    "llamados_v5['victima_edad_escalada'] = scaler.fit_transform(llamados_v5[['victima_edad']])\n",
    "llamados_v5['llamante_edad_escalada'] = scaler.fit_transform(llamados_v5[['llamante_edad']])\n",
    "\n",
    "\n",
    "\n",
    "# Cosa 2: pasar edad a categórica\n",
    "\n",
    "def categoria_edad (x):\n",
    "    if (x >= 0) and (x <= 11) :\n",
    "        return 'Niñez'\n",
    "    elif (x >= 12) and (x <=18):\n",
    "        return 'Adolescencia'\n",
    "    elif (x >= 19) and (x <=30):\n",
    "        return 'Juventud'\n",
    "    elif (x>=31) and (x<=65) :\n",
    "        return 'Vejez'\n",
    "    elif x>=66:\n",
    "        return 'Vejez mayor'\n",
    "    else:\n",
    "        return 'NS/NC'\n",
    "\n",
    "\n",
    "\n",
    "llamados_v5['victima_edad_cat'] = \\\n",
    "llamados_v5.victima_edad.apply(categoria_edad)\n",
    "llamados_v5['llamante_edad_cat'] = \\\n",
    "llamados_v5.llamante_edad.apply(categoria_edad)\n",
    "#llamados_v5.drop(['victima_edad','llamante_edad'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# sobra: victima_edad y llamante_edad    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.281250\n",
       "1             NaN\n",
       "2        0.458333\n",
       "3             NaN\n",
       "4             NaN\n",
       "           ...   \n",
       "19138         NaN\n",
       "19139    0.208333\n",
       "19140    0.552083\n",
       "19141    0.385417\n",
       "19142         NaN\n",
       "Name: llamante_edad_escalada, Length: 19143, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamados_v5.llamante_edad_escalada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Variables categóricas que serán encodeadas con ordinal: edad cat (ordinal), momento día (ordinal), \n",
    "# estación del año (ordinal).\n",
    "# Además de encodearlas con ordinal, luego voy a escalar ese encoding ordinal. \n",
    "# FUENTE Encoding_Methods_for_Categorical_Data.pdf\n",
    "\n",
    "# 4.1 pasar edad_cat, momento_dia (V5) y estacion_del_año (V5) a dummy con un encoder ordinal\n",
    "\n",
    "hot_encoder = OneHotEncoder()\n",
    "    \n",
    "\n",
    "llamados_v5['victima_edad_cat_encoded'] = encoder.fit_transform(llamados_v5['victima_edad_cat'])\n",
    "llamados_v5['llamante_edad_cat_encoded'] = encoder.fit_transform(llamados_v5['llamante_edad_cat'])\n",
    "#llamados_v5.drop(['victima_edad_cat', 'llamante_edad_cat'], axis=1, inplace=True)\n",
    "\n",
    "#sobra victima_edad_cat y llamante_edad_cat\n",
    "\n",
    "llamados_v5['momento_dummy'] = encoder.fit_transform(llamados_v5['momento_dia'])\n",
    "llamados_v5['estacion_dummy'] = encoder.fit_transform(llamados_v5['estacion_del_año'])\n",
    "#llamados_v5.drop(['estacion_del_año','momento_dia',], axis=1, inplace=True)\n",
    "\n",
    "#sobra momento_dia y estacion_del_año\n",
    "\n",
    "# 4.2 escalar las variables ordinales edad, momento del día y estación\n",
    "\n",
    "\n",
    "llamados_v5['victima_edad_dummy_scaled'] = scaler.fit_transform(llamados_v5[['victima_edad_cat_dummy']])\n",
    "llamados_v5['llamante_edad_dummy_scaled'] = scaler.fit_transform(llamados_v5[['llamante_edad_cat_dummy']])\n",
    "llamados_v5['momento_dummy_scaled'] = scaler.fit_transform(llamados_v5[['momento_dummy']])\n",
    "llamados_v5['estacion_dummy_scaled'] = scaler.fit_transform(llamados_v5[['estacion_dummy']])\n",
    "\n",
    "#llamados_v5.drop('momento_dummy', axis=1, inplace=True)\n",
    "#llamados_v5.drop('estacion_dummy', axis=1, inplace=True )\n",
    "\n",
    "#llamados_v5.drop('victima_edad_cat_dummy', axis=1, inplace=True)\n",
    "#llamados_v5.drop('llamante_edad_cat_dummy', axis=1, inplace=True )\n",
    "\n",
    "\n",
    "#sobra victima_edad_cat_dummy y llamante_edad_cat_dummy, momento_dummy y estacion_dummy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# algunas de mis features tienen cardinalidad altísima, el one hot no va a andar bien para esas y el \n",
    "# ordinal aunque anda bien introduce un sesgo de ordinalidad que no es verdadero para esas feature.\n",
    "# una posibilidad es ver qué tan relacionadas están esas features con la variable target y quizás \n",
    "# las puedo matar. \n",
    "\n",
    "\n",
    "# hacer for loop para encodera el resto de las categóricas TODAS MENOS TARGET\n",
    "# luego separo el test final que son los vacíos de target\n",
    "# luego encodeo target \n",
    "# label encoder es ordinal, one hot no. Sin embargo, one hot puede ser problemático con alta \n",
    "# feature cardinality. Chequeemos feature cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llamado_provincia                            25\n",
       "llamante_genero                               4\n",
       "llamante_vinculo                             16\n",
       "caso_judicializado                            3\n",
       "hecho_lugar                                  17\n",
       "victima_a_resguardo                           2\n",
       "victima_genero                                4\n",
       "victima_nacionalidad                          9\n",
       "victima_vinculo_agresor                      15\n",
       "victima_discapacidad                          3\n",
       "victima_convive_agresor                       2\n",
       "vs_tocamiento_sexual                          2\n",
       "vs_intento_tocamiento                         2\n",
       "vs_grooming                                   2\n",
       "vs_exhibicionismo                             2\n",
       "vs_obligacion_sacarse_fotos_pornograficas     2\n",
       "vs_acoso_sexual                               2\n",
       "vs_iniciacion_sexual_forzada_inducida         2\n",
       "vs_otra_forma_violencia_sexual                2\n",
       "vs_no_sabe_no_contesta                        2\n",
       "ofv_sentimiento_amenaza                       2\n",
       "ofv_amenaza_explicita                         2\n",
       "ofv_violencia_fisica                          2\n",
       "ofv_enganio_seduccion                         2\n",
       "ofv_grooming                                  2\n",
       "ofv_otra_forma_violencia                      2\n",
       "ofv_no_sabe_no_contesta                       2\n",
       "agresor_fam_no_fam                            4\n",
       "genero_agresor                                4\n",
       "agresor_conocido_no_conocido                  3\n",
       "tipo_vinculo_llamante                         5\n",
       "momento_dia                                   5\n",
       "estacion_del_año                              4\n",
       "vs_explotacion_sexual_group                   2\n",
       "vs_violacion_group                            2\n",
       "vs_tentativa_group                            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamados_v5.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien al principio la idea era que la primera prueba de svm fuera con el dataset completo, normalizado pero con poca o ninguna intervención en la construcción de variables; llegados a este punto, la cardinalidad de alta de estasvaraibles lleva a tomar la decisón de reducrilas sin antes correr el experimento con svm porque ya está probado en la literatura que alta cardinalidad con encoders tipo one hot es mala y el target o ordinal encoder que funcionan bien para alta cardinalidad no me convencve para etsas vaiables porque no hay ordinalidad que preserar y porque el target implica tener otros cuidados para no incurrir en data leackage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode features\n",
    "hot_encoder = OneHotEncoder()\n",
    "X_encoded = hot_encoder.fit_transform(X).toarray()\n",
    "\n",
    "# Encode target variable\n",
    "hot_encoder = OneHotEncoder()\n",
    "y_encoded = hot_encoder.fit_transform(y.values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacar todos los casos vacíos de convive, esos serán el dataset aparte final a predecir con el mejor modelo\n",
    "# encodear el resto del dataset, todo menos y_convive\n",
    "\n",
    "def one_hot_encoder(dataset):\n",
    "    columna_excluir = 'victima_convive_agresor'\n",
    "    columnas_cat = dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    columnas_cat.remove(columna_excluir)\n",
    "\n",
    "    # Apply one-hot encoding to object columns excluding the chosen one\n",
    "    df_encoded = pd.get_dummies(df, columns=object_columns)\n",
    "\n",
    "    # Display the DataFrame with one-hot encoded columns\n",
    "    print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 separar los na de convive en los nuevos datasets\n",
    "# donde prueba 1: dataset completo, limpio. Prueba 2: dataset con feature engineering. \n",
    "# Prueba 3: dataset con eliminación de variables poco informativas\n",
    "\n",
    "prueba_1_no_visto = llamados_v2[llamados_v2['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "prueba_1_train_test = llamados_v2[~llamados_v2['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "\n",
    "prueba_2_no_visto = llamados_v4[llamados_v4['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "prueba_2_train_test = llamados_v4[~llamados_v4['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "\n",
    "prueba_3_no_visto = llamados_v5[llamados_v5['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "prueba_3_train_test = llamados_v5[~llamados_v5['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_1_no_visto.to_excel(os.path.join(dataset_dir, 'xlsx/prueba_1_no_visto.xlsx'), index=False)\n",
    "prueba_2_no_visto.to_excel(os.path.join(dataset_dir, 'xlsx/prueba_2_no_visto.xlsx'), index=False)\n",
    "prueba_3_no_visto.to_excel(os.path.join(dataset_dir, 'xlsx/prueba_3_no_visto.xlsx'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llamados_v2, llamados_v4, llamados_v5, prueba_1_no_visto, prueba_2_no_visto, prueba_3_no_visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 separar features de target\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "# X\n",
    "\n",
    "prueba_1_X = prueba_1_train_test.drop('victima_convive_agresor', axis=1)\n",
    "prueba_2_X = prueba_2_train_test.drop('victima_convive_agresor', axis=1)\n",
    "prueba_3_X = prueba_3_train_test.drop('victima_convive_agresor', axis=1)\n",
    "\n",
    "# Y\n",
    "\n",
    "prueba_1_y = prueba_1_train_test['victima_convive_agresor']\n",
    "prueba_2_y = prueba_2_train_test['victima_convive_agresor']\n",
    "prueba_3_y = prueba_3_train_test['victima_convive_agresor']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos usando los datasets preparados para svm:\n",
    "\n",
    "- las variaciones son en las columnas de edad, en los datasets que tengo, y en los kernels de SVM\n",
    "\n",
    "##### Prueba 1, dataset V2, V3, V4\n",
    "- edad categórica pasada a dummy se va\n",
    "- edad numérica se queda pero solo los datos completos de ambas\n",
    "- corro SVN c =/= kernels\n",
    "\n",
    "##### Prueba 2, dataset V2, V3, V4\n",
    "- edad categórica pasada a dummy se va\n",
    "- edad numérica se queda pero solo los datos completos de victima\n",
    "- corro SVN c =/= kernels\n",
    "\n",
    "##### Prueba 3, dataset V2, V3, V4\n",
    "- edad categórica pasada a dummy se va\n",
    "- edad numérica se queda pero solo los datos completos de llamante\n",
    "- corro SVN c =/= kernels\n",
    "\n",
    "##### Prueba 4, dataset V2, V3, V4\n",
    "- edad numérica se va\n",
    "- edad categórica pasada a dummy se queda\n",
    "- corro SVN c =/= kernels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
