{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), 'datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datos a usar\n",
    "\n",
    "- V2 --> dataset limpio\n",
    "- V4 --> variables construidas a partir de otras + agrupacion de violencias absorbidas\n",
    "- V5 --> eliminación de las variables cn poca información\n",
    "\n",
    "\n",
    "##### Pipeline\n",
    "\n",
    "1. cargar dataset\n",
    "2. reemplazar NS/NC en convive por NA\n",
    "3. hacer modificaciones para svm. Considerar: Apply scikit's OneHotEncoder with the handle_unknown parameter set to \"ignore\"\n",
    "4. separar NA de convive, serán el dataset \"no visto\"\n",
    "5. separar features de target\n",
    "6. separar train-test NO DEBERÍA HACER LAS MODIFICACIONES EN TRAIN Y TEST POR SEPARADO?\n",
    "7. armar modelo\n",
    "8. evaluar\n",
    "\n",
    "\n",
    "##### Modificaiones para SVM\n",
    "\n",
    "1. pasar datetime a timestamp y escalar --> OK\n",
    "2. escalar la variable edad --> OK\n",
    "3. pasar edad a categórica (en una columna distinta) --> OK\n",
    "4. pasar todas las categóricas a dummy:\n",
    "   1. edad, momento del día y estación del año con un encoder ordinal\n",
    "   2. escalar las features encodeadas con ordinal\n",
    "   3. reducir la cardinalidad de las variables hecho lugar, victima vinculo agresor, provincia y vinculo llamante \n",
    "   4. el resto con one hot encoder\n",
    "5. borrar las categóricas para hacer más pequeño el dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 cargar datasets\n",
    "\n",
    "llamados_v2= pd.read_excel(os.path.join(dataset_dir, 'xlsx/llamados_v2.xlsx'), parse_dates=['llamado_fecha_hora'])\n",
    "llamados_v4= pd.read_excel(os.path.join(dataset_dir, 'xlsx/llamados_v4.xlsx'), parse_dates=['llamado_fecha_hora'])\n",
    "llamados_v5= pd.read_excel(os.path.join(dataset_dir, 'xlsx/llamados_v5.xlsx'), parse_dates=['llamado_fecha_hora'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "todos_datasets = [llamados_v2, llamados_v4, llamados_v5]\n",
    "datasets_45 = [llamados_v4, llamados_v5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 reemplazar todos los NS/NC en convive por na\n",
    "for dataset in todos_datasets:\n",
    "\n",
    "    # Replace the specified values with NaN\n",
    "    dataset.loc[:, 'victima_convive_agresor'] = dataset['victima_convive_agresor'].replace({'NS/NC': pd.NA})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1        0.000060\n",
       "2        0.000121\n",
       "3        0.000181\n",
       "4        0.000181\n",
       "           ...   \n",
       "19138    0.999758\n",
       "19139    0.999940\n",
       "19140    0.999819\n",
       "19141    0.999879\n",
       "19142    0.999879\n",
       "Name: timestamp_encoded_scaled, Length: 19143, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "\n",
    "llamados_v2['timestamp_encoded'] = encoder.fit_transform(llamados_v2[['llamado_fecha_hora']])\n",
    "llamados_v2['timestamp_encoded_scaled'] = scaler.fit_transform(llamados_v2[['timestamp_encoded']])\n",
    "    \n",
    "llamados_v2.timestamp_encoded_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.000000\n",
       "1        0.000024\n",
       "2        0.000135\n",
       "3        0.000275\n",
       "4        0.000275\n",
       "           ...   \n",
       "19138    0.999840\n",
       "19139    0.999941\n",
       "19140    0.999885\n",
       "19141    0.999921\n",
       "19142    0.999921\n",
       "Name: timestamp_scaled, Length: 19143, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "llamados_v2['timestamp_scaled'] = scaler.fit_transform(llamados_v2[['llamado_fecha_hora']])\n",
    "llamados_v2.timestamp_scaled # sin encodear primero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 modificaciones para svm parte 1\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "hot_encoder = OneHotEncoder()\n",
    "\n",
    "# Las varaibles numéricas que tengo son: fechahora y edad. Edad tiene datos faltantes. \n",
    "# A fechahora la escalo.  \n",
    "# A edad le hago dos cosas para dos pruebas distintas: \n",
    "# cosa 1. escalo los casos completos.\n",
    "# cosa 2: la paso a categórica, ahí los faltantes quedan codificaos como una categoría más, y ahí luego la encodeo como \n",
    "# variable categórica con un ordina encoder para preservar el orden. \n",
    "\n",
    "# 1. fecha hora escalado\n",
    "\n",
    "for dataset in todos_datasets:\n",
    "    dataset['timestamp_scaled'] = scaler.fit_transform(dataset[['llamado_fecha_hora']])\n",
    "    dataset.drop('llamado_fecha_hora', axis=1, inplace=True )\n",
    "\n",
    "# 2. crear una columna con edad escalada para las pruebas en que use solo los casos completos de edad\n",
    "    # printear una edad faltante a ver qué hace con esos faltantes\n",
    "for dataset in todos_datasets:\n",
    "    dataset['victima_edad_escalada'] = scaler.fit_transform(dataset[['victima_edad']])\n",
    "    dataset['llamante_edad_escalada'] = scaler.fit_transform(dataset[['llamante_edad']])\n",
    "\n",
    "\n",
    "\n",
    "# 3. pasar edad a categórica\n",
    "\n",
    "\n",
    "def categoria_edad (x):\n",
    "    if (x >= 0) and (x <= 11) :\n",
    "        return 'Niñez'\n",
    "    elif (x >= 12) and (x <=18):\n",
    "        return 'Adolescencia'\n",
    "    elif (x >= 19) and (x <=30):\n",
    "        return 'Juventud'\n",
    "    elif (x>=31) and (x<=65) :\n",
    "        return 'Vejez'\n",
    "    elif x>=66:\n",
    "        return 'Vejez mayor'\n",
    "    else:\n",
    "        return 'NS/NC'\n",
    "\n",
    "for dataset in todos_datasets:\n",
    "    dataset['victima_edad_cat'] = \\\n",
    "        dataset.victima_edad.apply(categoria_edad)\n",
    "    dataset['llamante_edad_cat'] = \\\n",
    "        dataset.llamante_edad.apply(categoria_edad)\n",
    "    dataset.drop(['victima_edad','llamante_edad'], axis=1, inplace=True)\n",
    "\n",
    "# 4. Variables categóricas que serán encodeadas con ordinal: edad cat (ordinal), momento día (ordinal), \n",
    "# estación del año (ordinal).\n",
    "# Además de encodearlas con ordinal, luego voy a escalar ese encoding ordinal. \n",
    "# FUENTE Encoding_Methods_for_Categorical_Data.pdf\n",
    "\n",
    "# 4.1 pasar edad_cat, momento_dia (V4, V5) y estacion_del_año (V4, V5) a dummy con un encoder ordinal\n",
    "\n",
    "\n",
    "    \n",
    "for dataset in todos_datasets:\n",
    "    dataset['victima_edad_cat_dummy'] = ordinal_encoder.fit_transform(dataset['victima_edad_cat'])\n",
    "    dataset['llamante_edad_cat_dummy'] = ordinal_encoder.fit_transform(dataset['llamante_edad_cat'])\n",
    "    dataset.drop(['victima_edad_cat', 'llamante_edad_cat'], axis=1, inplace=True)\n",
    "\n",
    "for dataset in datasets_45:\n",
    "    dataset['momento_dummy'] = ordinal_encoder.fit_transform(dataset['momento_dia'])\n",
    "    dataset['estacion_dummy'] = ordinal_encoder.fit_transform(dataset['estacion_del_año'])\n",
    "    dataset.drop(['estacion_del_año','momento_dia',], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# 4.2 escalar las variables ordinales edad, momento del día y estación\n",
    "\n",
    "# edad existe en todos los datasets\n",
    "for dataset in todos_datasets:\n",
    "    dataset['victima_edad_dummy_scaled'] = scaler.fit_transform(dataset[['victima_edad_cat_dummy']])\n",
    "    dataset['llamante_edad_dummy_scaled'] = scaler.fit_transform(dataset[['llamante_edad_cat_dummy']])\n",
    "    dataset.drop('victima_edad_cat_dummy', axis=1, inplace=True)\n",
    "    dataset.drop('llamante_edad_cat_dummy', axis=1, inplace=True )\n",
    "\n",
    "# estación y momento del día solo existen en los datasets 4 y 5\n",
    "for dataset in datasets_45:\n",
    "    dataset['momento_dummy_scaled'] = scaler.fit_transform(dataset[['momento_dummy']])\n",
    "    dataset['estacion_dummy_scaled'] = scaler.fit_transform(dataset[['estacion_dummy']])\n",
    "    dataset.drop('momento_dummy', axis=1, inplace=True)\n",
    "    dataset.drop('estacion_dummy', axis=1, inplace=True )\n",
    "\n",
    "# algunas de mis features tienen cardinalidad altísima, el one hot no va a andar bien para esas y el \n",
    "# ordinal aunque anda bien introduce un sesgo de ordinalidad que no es verdadero para esas feature.\n",
    "# una posibilidad es ver qué tan relacionadas están esas features con la variable target y quizás \n",
    "# las puedo matar. \n",
    "\n",
    "\n",
    "# hacer for loop para encodera el resto de las categóricas TODAS MENOS TARGET\n",
    "# luego separo el test final que son los vacíos de target\n",
    "# luego encodeo target \n",
    "# label encoder es ordinal, one hot no. Sin embargo, one hot puede ser problemático con alta \n",
    "# feature cardinality. Chequeemos feature cardinality\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llamado_provincia                            25\n",
       "llamante_genero                               4\n",
       "llamante_vinculo                             16\n",
       "caso_judicializado                            3\n",
       "hecho_lugar                                  17\n",
       "victima_a_resguardo                           2\n",
       "victima_genero                                4\n",
       "victima_nacionalidad                          9\n",
       "victima_vinculo_agresor                      15\n",
       "victima_discapacidad                          3\n",
       "victima_convive_agresor                       2\n",
       "vs_tocamiento_sexual                          2\n",
       "vs_intento_tocamiento                         2\n",
       "vs_grooming                                   2\n",
       "vs_exhibicionismo                             2\n",
       "vs_obligacion_sacarse_fotos_pornograficas     2\n",
       "vs_acoso_sexual                               2\n",
       "vs_iniciacion_sexual_forzada_inducida         2\n",
       "vs_otra_forma_violencia_sexual                2\n",
       "vs_no_sabe_no_contesta                        2\n",
       "ofv_sentimiento_amenaza                       2\n",
       "ofv_amenaza_explicita                         2\n",
       "ofv_violencia_fisica                          2\n",
       "ofv_enganio_seduccion                         2\n",
       "ofv_grooming                                  2\n",
       "ofv_otra_forma_violencia                      2\n",
       "ofv_no_sabe_no_contesta                       2\n",
       "agresor_fam_no_fam                            4\n",
       "genero_agresor                                4\n",
       "agresor_conocido_no_conocido                  3\n",
       "tipo_vinculo_llamante                         5\n",
       "momento_dia                                   5\n",
       "estacion_del_año                              4\n",
       "vs_explotacion_sexual_group                   2\n",
       "vs_violacion_group                            2\n",
       "vs_tentativa_group                            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamados_v5.select_dtypes(include=['object']).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llamado_provincia\n",
       "CABA                   7083\n",
       "Buenos Aires           6896\n",
       "NS/NC                  1793\n",
       "Córdoba                 694\n",
       "Santa Fe                545\n",
       "Mendoza                 365\n",
       "Tucumán                 244\n",
       "Salta                   172\n",
       "Entre Ríos              148\n",
       "Neuquén                 130\n",
       "Río Negro               123\n",
       "Misiones                105\n",
       "Corrientes               99\n",
       "Jujuy                    99\n",
       "Chubut                   87\n",
       "Chaco                    86\n",
       "San Luis                 80\n",
       "Santiago del Estero      76\n",
       "Formosa                  65\n",
       "Santa Cruz               57\n",
       "Catamarca                56\n",
       "Tierra del Fuego         48\n",
       "San Juan                 44\n",
       "La Rioja                 32\n",
       "La Pampa                 16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data donde catgorical = unique\n",
    "llamados_v2['llamado_provincia'].value_counts()\n",
    "# llamado provincia, llamante vinculo, hecho lugar, victima vinculo agresor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hecho_lugar\n",
       "NS/NC                         5675\n",
       "Vivienda de la Víctima        4787\n",
       "Vivienda del Agresor          2635\n",
       "Redes Sociales                2329\n",
       "Otro                          1085\n",
       "Calle                          717\n",
       "Vivienda de un familiar        627\n",
       "Ámbito educativo               426\n",
       "Comercio                       239\n",
       "Subterráneo/Tren/Colectivo     188\n",
       "Automóvil                      171\n",
       "Albergue transitorio            76\n",
       "Plaza                           64\n",
       "Descampado                      54\n",
       "Taxi                            29\n",
       "Residencia turística            28\n",
       "Obra en construcción            13\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data donde catgorical = unique\n",
    "llamados_v2['hecho_lugar'].value_counts()\n",
    "# llamado provincia, llamante vinculo, hecho lugar, victima vinculo agresor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hay 4 variables con altísima cardinalidad, que me va a joder en el one hot encoder\n",
    "Soluciones posibles:\n",
    "- las achico, menos cardinalidad agrupando. Eso lo hice ya con llamante y agresor vinculo, podría hacerlo con hecho lugar y con provincia. \n",
    "- hecho lugar lo puedo agrupar por  vivienda victima - vivienda agresor - vivienda familiar - ns/nc - otro \n",
    "- provincia se podría agrupar por zona del país: noroeste, noreste, sur, este, oeste \n",
    "- Y se puede explicar así: si bien al principio la idea era que la primera prueba de svm fuera con el dataset completo, normalizado pero con poca o ninguna intervención en la construcción de variables; llegados a este punto, la cardinalidad de alta de estasvaraibles lleva a tomar la decisón de reducrilas sin antes correr el experimento con svm porque ya está probado en la literatura que alta cardinalidad con encoders tipo one hot es mala y el target o ordinal encoder que funcionan bien para alta cardinalidad no me convencve para etsas vaiables porque no hay ordinalidad que preserar y porque el target implica tener otros cuidados para no incurrir en data leackage\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode features\n",
    "hot_encoder = OneHotEncoder()\n",
    "X_encoded = hot_encoder.fit_transform(X).toarray()\n",
    "\n",
    "# Encode target variable\n",
    "hot_encoder = OneHotEncoder()\n",
    "y_encoded = hot_encoder.fit_transform(y.values.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sacar todos los casos vacíos de convive, esos serán el dataset aparte final a predecir con el mejor modelo\n",
    "# encodear el resto del dataset, todo menos y_convive\n",
    "\n",
    "def one_hot_encoder(dataset):\n",
    "    columna_excluir = 'victima_convive_agresor'\n",
    "    columnas_cat = dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "    columnas_cat.remove(columna_excluir)\n",
    "\n",
    "    # Apply one-hot encoding to object columns excluding the chosen one\n",
    "    df_encoded = pd.get_dummies(df, columns=object_columns)\n",
    "\n",
    "    # Display the DataFrame with one-hot encoded columns\n",
    "    print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 separar los na de convive en los nuevos datasets\n",
    "# donde prueba 1: dataset completo, limpio. Prueba 2: dataset con feature engineering. \n",
    "# Prueba 3: dataset con eliminación de variables poco informativas\n",
    "\n",
    "prueba_1_no_visto = llamados_v2[llamados_v2['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "prueba_1_train_test = llamados_v2[~llamados_v2['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "\n",
    "prueba_2_no_visto = llamados_v4[llamados_v4['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "prueba_2_train_test = llamados_v4[~llamados_v4['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "\n",
    "prueba_3_no_visto = llamados_v5[llamados_v5['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "prueba_3_train_test = llamados_v5[~llamados_v5['victima_convive_agresor'].isna()].copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_1_no_visto.to_excel(os.path.join(dataset_dir, 'xlsx/prueba_1_no_visto.xlsx'), index=False)\n",
    "prueba_2_no_visto.to_excel(os.path.join(dataset_dir, 'xlsx/prueba_2_no_visto.xlsx'), index=False)\n",
    "prueba_3_no_visto.to_excel(os.path.join(dataset_dir, 'xlsx/prueba_3_no_visto.xlsx'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llamados_v2, llamados_v4, llamados_v5, prueba_1_no_visto, prueba_2_no_visto, prueba_3_no_visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 separar features de target\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "# X\n",
    "\n",
    "prueba_1_X = prueba_1_train_test.drop('victima_convive_agresor', axis=1)\n",
    "prueba_2_X = prueba_2_train_test.drop('victima_convive_agresor', axis=1)\n",
    "prueba_3_X = prueba_3_train_test.drop('victima_convive_agresor', axis=1)\n",
    "\n",
    "# Y\n",
    "\n",
    "prueba_1_y = prueba_1_train_test['victima_convive_agresor']\n",
    "prueba_2_y = prueba_2_train_test['victima_convive_agresor']\n",
    "prueba_3_y = prueba_3_train_test['victima_convive_agresor']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos usando los datasets preparados para svm:\n",
    "\n",
    "- las variaciones son en las columnas de edad, en los datasets que tengo, y en los kernels de SVM\n",
    "\n",
    "##### Prueba 1, dataset V2, V3, V4\n",
    "- edad categórica pasada a dummy se va\n",
    "- edad numérica se queda pero solo los datos completos de ambas\n",
    "- corro SVN c =/= kernels\n",
    "\n",
    "##### Prueba 2, dataset V2, V3, V4\n",
    "- edad categórica pasada a dummy se va\n",
    "- edad numérica se queda pero solo los datos completos de victima\n",
    "- corro SVN c =/= kernels\n",
    "\n",
    "##### Prueba 3, dataset V2, V3, V4\n",
    "- edad categórica pasada a dummy se va\n",
    "- edad numérica se queda pero solo los datos completos de llamante\n",
    "- corro SVN c =/= kernels\n",
    "\n",
    "##### Prueba 4, dataset V2, V3, V4\n",
    "- edad numérica se va\n",
    "- edad categórica pasada a dummy se queda\n",
    "- corro SVN c =/= kernels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
